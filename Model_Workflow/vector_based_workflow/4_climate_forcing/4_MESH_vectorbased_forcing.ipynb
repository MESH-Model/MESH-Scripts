{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASYMORE Basin Subset\n",
    "EASYMORE: EArth SYstem MOdeling REmapper is a collection of functions that allows extraction of the data from a NetCDF file for a given shapefile such as a basin, catchment, points or lines. It can map gridded data or model output to any given shapefile and provide area average for a target variable.\n",
    "https://github.com/ShervanGharari/EASYMORE\n",
    "\n",
    "## Climate Forcing Reordering\n",
    "The purpose of this script is to extract vector-based forcing files remapped from easymore and reorder them based rank, then save them to a netcdf format that can be read by MESH model\n",
    "\n",
    "#### **Programmer**\n",
    "Ala Bahrami\n",
    "\n",
    "#### **Revision History**\n",
    "2021/05/13 -- (1) Initial version created and posted online<br>\n",
    "2022/06/26 -- (1) Instead of reading the shape file, the MeritHydro subbasin metadata is read as an input for reindexing forcing.<br> \n",
    "2022/06/26 -- (2) Changed variable names to match RDRSV2.1<br>\n",
    "\n",
    "#### **See Also**\n",
    "easymore_extarct, create_MESH_drainage_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xs\n",
    "import time \n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import netCDF4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control File Handling\n",
    "The purpose of the control file is to provide all inputs to the scripts in the vector-based workflow to eliminate the need to alter the workflow scripts themselves. The following cells will retrieve settings from 'control_active.txt' and provide them as inputs to this script."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Access to the control file folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlFolder = Path('../0_control_files')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store the name of the 'active' file in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlFile = 'control_active.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to extract a given setting from the control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_control( file, setting ):\n",
    "     \n",
    "    # Open 'control_active.txt' and ...\n",
    "    with open(file) as contents:\n",
    "        for line in contents:\n",
    "             \n",
    "            # ... find the line with the requested setting\n",
    "            if setting in line and not line.startswith('#'):\n",
    "                break\n",
    "     \n",
    "    # Extract the setting's value\n",
    "    substring = line.split('|',1)[1]      # Remove the setting's name (split into 2 based on '|', keep only 2nd part)\n",
    "    substring = substring.split('#',1)[0] # Remove comments, does nothing if no '#' is found\n",
    "    substring = substring.strip()         # Remove leading and trailing whitespace, tabs, newlines\n",
    "        \n",
    "    # Return this value   \n",
    "    return substring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to specify a default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_path(suffix):\n",
    "     \n",
    "    # Get the root path\n",
    "    rootPath = Path( read_from_control(controlFolder/controlFile,'root_path') )\n",
    "     \n",
    "    # Specify the forcing path\n",
    "    #defaultPath = rootPath / domainFolder / suffix\n",
    "    defaultPath = rootPath / suffix \n",
    "    return defaultPath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the domain folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_name = read_from_control(controlFolder/controlFile,'domain_name')\n",
    "domainFolder = 'domain_' + domain_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get additional settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset     =  read_from_control(controlFolder/controlFile,'forcing_dataset')\n",
    "startTime   = read_from_control(controlFolder/controlFile,'forcing_start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_dir  = read_from_control(controlFolder/controlFile,'remapping_out')\n",
    "if forcing_dir == 'default':\n",
    "    forcing_dir = make_default_path('forcing') # outputs a Path()\n",
    "else:\n",
    "    forcing_dir = forcing_dir # make sure a user-specified path is a Path()\n",
    "\n",
    "forcing_dataset = read_from_control(controlFolder/controlFile,'forcing_dataset')\n",
    "forcing_name = '{}_{}_remapped_{}.nc'.format(dataset,domain_name,startTime)\n",
    "ddb_folder   = read_from_control(controlFolder/controlFile,'DDB_output_dir')\n",
    "if ddb_folder == 'default':\n",
    "    ddb_folder = make_default_path('vector_based_workflow/workflow_data/domain_{}/drainagedatabase'.format(domain_name)) # outputs a Path()\n",
    "else:\n",
    "    ddb_folder = ddb_folder # make sure a user-specified path is a Path()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading input basin \n",
    "*the segids are stored in the remapped forcing, so it is not necessary to read input shape file*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the drainage database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "db = xs.open_dataset(ddb_folder / '{}_MESH_drainage_database.nc'.format(domain_name))\n",
    "db.close()\n",
    "segid =  db.variables['seg_id'].values\n",
    "# reading for control check \n",
    "lon = db.variables['lon'].values\n",
    "lat = db.variables['lat'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the input forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/5600x/Desktop/GWF/MESH-Scripts/Model_Workflow/forcing/RDRS_BowAtBanff_remapped_1980-01-01-13-00-00.nc')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forcing_dir / forcing_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forc = xs.open_dataset(forcing_dir / forcing_name)\n",
    "forc.close()\n",
    "lon_ease = forc.variables['longitude'].values\n",
    "lat_ease = forc.variables['latitude'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract indices of forcing ids based on the drainage database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(segid)\n",
    "ind = []\n",
    "\n",
    "for i in range(n):\n",
    "    fid = np.where(np.int32(forc['ID'].values) == segid[i])[0]\n",
    "    ind = np.append(ind, fid)\n",
    "\n",
    "ind = np.int32(ind) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reorder input forcing\n",
    "###### *Note : name of variables is hard coded and can be modified regarding the input forcing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0.25155115127563477 seconds--\n"
     ]
    }
   ],
   "source": [
    "forc_vec = xs.Dataset(\n",
    "    {\n",
    "        \"RDRS_v2.1_A_PR0_SFC\": ([\"subbasin\", \"time\"], forc['RDRS_v2.1_A_PR0_SFC'].values[:,ind].transpose()),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": forc['time'].values.copy(),\n",
    "        \"lon\": ([\"subbasin\"], lon),\n",
    "        \"lat\": ([\"subbasin\"], lat),\n",
    "    }\n",
    "    )\n",
    "\n",
    "forc_vec['RDRS_v2.1_A_PR0_SFC'].encoding['coordinates'] = 'time lon lat'\n",
    "forc_vec['RDRS_v2.1_A_PR0_SFC'].attrs[\"units\"]          = forc['RDRS_v2.1_A_PR0_SFC'].units\n",
    "forc_vec['RDRS_v2.1_A_PR0_SFC'].attrs[\"grid_mapping\"]   = 'crs'\n",
    "\n",
    "for n in ['RDRS_v2.1_P_FI_SFC','RDRS_v2.1_P_FB_SFC','RDRS_v2.1_P_TT_09944',\n",
    "          'RDRS_v2.1_P_UVC_09944','RDRS_v2.1_P_P0_SFC','RDRS_v2.1_P_HU_09944']:\n",
    "    forc_vec[n] = ((\"subbasin\", \"time\"), forc[n].values[: , ind].transpose()) \n",
    "    forc_vec[n].coords[\"time\"]          = forc['time'].values.copy()\n",
    "    forc_vec[n].coords[\"lon\"]           = ([\"subbasin\"], lon)\n",
    "    forc_vec[n].coords[\"lat\"]           = ([\"subbasin\"], lat)\n",
    "    forc_vec[n].attrs[\"units\"]          = forc[n].units\n",
    "    forc_vec[n].attrs[\"grid_mapping\"]   = 'crs'\n",
    "    forc_vec[n].encoding['coordinates'] = 'time lon lat'\n",
    "\n",
    "# %% update meta data attribuetes \n",
    "forc_vec.attrs['Conventions'] = 'CF-1.6'\n",
    "forc_vec.attrs['License']     = 'The data were written by Ala Bahrami'\n",
    "forc_vec.attrs['history']     = 'Created on June 07, 2021'\n",
    "forc_vec.attrs['featureType'] = 'timeSeries'          \n",
    "\n",
    "# editing lat attribute\n",
    "forc_vec['lat'].attrs['standard_name'] = 'latitude'\n",
    "forc_vec['lat'].attrs['units']         = 'degrees_north'\n",
    "forc_vec['lat'].attrs['axis']          = 'Y'\n",
    " \n",
    "# editing lon attribute\n",
    "forc_vec['lon'].attrs['standard_name'] = 'longitude'\n",
    "forc_vec['lon'].attrs['units']         = 'degrees_east'\n",
    "forc_vec['lon'].attrs['axis']          = 'X'\n",
    "\n",
    "# editing time attribute\n",
    "forc_vec['time'].attrs['standard_name'] = 'time'\n",
    "forc_vec['time'].attrs['axis']          = 'T'\n",
    "forc_vec['time'].encoding['calendar']   = 'gregorian'\n",
    "forc_vec.encoding.update(unlimited_dims = 'time')\n",
    "\n",
    "# coordinate system\n",
    "forc_vec['crs'] = db['crs'].copy()\n",
    "\n",
    "# Define a variable for the points and set the 'timeseries_id' (required for some viewers).\n",
    "forc_vec['subbasin'] = (['subbasin'], db['seg_id'].values.astype(np.int32).astype('S20'))\n",
    "forc_vec['subbasin'].attrs['cf_role'] = 'timeseries_id'\n",
    "\n",
    "#%% save to netcdf \n",
    "comp = dict(zlib=True, complevel=6)\n",
    "encoding = {var: comp for var in forc_vec.data_vars}\n",
    "forc_vec.to_netcdf(forcing_dir / 'MESH_{}'.format(forcing_name),encoding=encoding)\n",
    "print('--%s seconds--' %(time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Provenance\n",
    "Generates a basic log file in the domain folder and copies the control file and itself there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log path and file name\n",
    "logPath = Path(forcing_dir)\n",
    "log_suffix = '_MESH_vectorbased_forcing.txt'\n",
    " \n",
    "# Create a log folder\n",
    "logFolder = '_workflow_log'\n",
    "Path( logPath / logFolder ).mkdir(parents=True, exist_ok=True)\n",
    " \n",
    "# Copy this script\n",
    "thisFile = '4_MESH_vectorbased_forcing.ipynb'\n",
    "copyfile(thisFile, logPath / logFolder / thisFile);\n",
    " \n",
    "# Get current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "# Create a log file\n",
    "logFile = now.strftime('%Y%m%d') + log_suffix\n",
    "with open( logPath / logFolder / logFile, 'w') as file:\n",
    "     \n",
    "    lines = ['Log generated by ' + thisFile + ' on ' + now.strftime('%Y/%m/%d %H:%M:%S') + '\\n',\n",
    "             'Generated remapped climate forcing .nc file, reordered to MESH requirements.']\n",
    "    for txt in lines:\n",
    "        file.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b271b39875848de38483eb62c29138594d92c4575023a90ae6ab83d843ef02c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
